{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOwPRcBa2gXgtJnkAi9ruvy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sameer-30/Neural-Network-From-Scratch/blob/main/Building_a_Multi_Layer_Neural_Network_from_Scratch_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dense Neural Networks\n",
        "\n",
        "In deep neural networks, hidden layers are the layers between the input and output. Even though these layers are called \"hidden\" because we don't directly observe their values, they perform essential transformations on the data. In a dense (fully-connected) layer, every neuron is connected to every input feature.\n",
        "\n",
        "The basic transformation in a dense layer is given by the equation:\n",
        "\n",
        "$$\n",
        "\\text{Output} = X \\cdot W + b\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $X$ is the input matrix (with shape (samples, features))\n",
        ".\n",
        "- $W$ is the weight matrix (with shape (features, neurons)).\n",
        "- $b$ is the bias vector (with shape (1, neurons)).\n",
        "\n",
        "Each neuron computes a weighted sum of the inputs and adds a bias term. The weights are typically initialized with small random values to break symmetry, and biases can be initialized to small non-zero values if desired.\n"
      ],
      "metadata": {
        "id": "-uSjVIAfE47y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building a Dense Neural Network from Scratch"
      ],
      "metadata": {
        "id": "mL-0qkj-GRES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import numpy for numerical operations\n",
        "import numpy as np\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define the DenseLayer class\n",
        "class DenseLayer:\n",
        "    def __init__(self, n_inputs, n_neurons, weight_scale=0.05):\n",
        "\n",
        "        # Initialize weights with small random numbers from a Gaussian distribution.\n",
        "        self.weights = weight_scale * np.random.randn(n_inputs, n_neurons)\n",
        "\n",
        "        # Initialize biases with small random values.\n",
        "        # Though biases are often set to zero, small non-zero values can help in early learning stages.\n",
        "        self.biases = np.random.randn(1, n_neurons) * 0.1\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        # Perform the forward pass: compute the dot product between inputs and weights, then add biases.\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "# Using the DenseLayer class\n",
        "\n",
        "# Generate some dummy input data:\n",
        "# Let's assume we have 5 samples, each with 4 features.\n",
        "X = np.random.rand(5, 4)\n",
        "\n",
        "# Create an instance of DenseLayer with 4 input features and 3 neurons.\n",
        "layer1 = DenseLayer(n_inputs=4, n_neurons=3)\n",
        "\n",
        "# Perform a forward pass to compute the outputs for our input data.\n",
        "layer1.forward(X)\n",
        "\n",
        "# Display the output from the dense layer.\n",
        "print(\"Output of the DenseLayer:\")\n",
        "print(layer1.output)\n"
      ],
      "metadata": {
        "id": "oGx7WUOUGEl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Explanation of the Code\n",
        "\n",
        "### 1. Import and Seed Initialization\n",
        "- We first import the NumPy library, which is essential for numerical computations.\n",
        "- The `np.random.seed(42)` function is used to ensure reproducibility. This makes sure that every time we run the code, we get the same random values.\n",
        "\n",
        "### 2. DenseLayer Class Definition\n",
        "#### (a) Constructor (`__init__`)\n",
        "- The `DenseLayer` class represents a fully connected layer in a neural network.\n",
        "- Weight Initialization:\n",
        "  - The weights `self.weights` are initialized using a Gaussian distribution scaled by `weight_scale`. This ensures the initial weights are small.\n",
        "  - The shape of `self.weights` is $\\text{n_inputs}, \\text{n_neurons}$, meaning each neuron has a weight for every input feature.\n",
        "- Bias Initialization:\n",
        "  - The biases `self.biases` are initialized with small random values. They have a shape of $(1, \\text{n_neurons})$.\n",
        "- The mathematical operation performed in this layer follows:\n",
        "\n",
        "  \n",
        "  $$\\text{Output} = X \\cdot W + b$$\n",
        "\n",
        "#### (b) Forward Method (`forward`)\n",
        "- The `forward` method takes an input matrix and computes the dot product with `self.weights`, then adds `self.biases`.\n",
        "- The result is stored in `self.output`, which represents the layerâ€™s output values.\n",
        "\n",
        "### 3. Using the DenseLayer\n",
        "#### (a) Creating Dummy Data\n",
        "- We generate a random input matrix $X$ with shape \\( (5, 4) \\), meaning 5 samples and 4 features per sample.\n",
        "\n",
        "#### (b) Instantiating the Dense Layer\n",
        "- We create an instance of `DenseLayer` with 4 input features and 3 neurons.\n",
        "\n",
        "#### (c) Performing a Forward Pass\n",
        "- We call `layer1.forward(X)`, which computes the output of the layer using the equation:\n",
        "\n",
        "  $$\\text{Output} = X \\cdot W + b$$\n",
        "\n",
        "- The computed output is then printed.\n",
        "\n",
        "This example demonstrates how a dense layer transforms input data into output using basic matrix operations. This forms the foundation for deeper neural networks.\n"
      ],
      "metadata": {
        "id": "Aa3Hhk9aHSyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Multiple Layers to the Neural Network\n",
        "\n",
        "So far, we have implemented a single dense layer that takes inputs and computes an output. However, in deep neural networks, we typically have multiple hidden layers that allow the network to learn complex patterns.\n",
        "\n",
        "To extend our previous example, we will:\n",
        "1. Add a second dense layer to process the output of the first layer.\n",
        "2. Ensure that the number of inputs to the second layer matches the number of neurons in the first layer.\n",
        "3. Compute outputs sequentially, passing data from one layer to the next.\n",
        "\n",
        "## Mathematical Formulation\n",
        "For a neural network with two layers, the computations are:\n",
        "\n",
        "$$\\text{Layer 1 Output} = X \\cdot W_1 + b_1$$\n",
        "\n",
        "$$\\text{Layer 2 Output} = \\text{Layer 1 Output} \\cdot W_2 + b_2$$\n",
        "\n",
        "where:\n",
        "- $ X $ is the input matrix.\n",
        "- $W_1, b_1 $ are the weights and biases of the first layer.\n",
        "- $ W_2, b_2 $ are the weights and biases of the second layer.\n",
        "- The output of ***Layer 1*** serves as the input to ***Layer 2***.\n"
      ],
      "metadata": {
        "id": "AuYUdV8LJPz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import numpy for numerical operations\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define a Dense Layer class\n",
        "class DenseLayer:\n",
        "    def __init__(self, n_inputs, n_neurons, weight_scale=0.05):\n",
        "\n",
        "        self.weights = weight_scale * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.random.randn(1, n_neurons) * 0.1\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "# Generate some dummy input data\n",
        "X = np.random.rand(5, 4)  # 5 samples, 4 features per sample\n",
        "\n",
        "# Create the first dense layer (4 inputs -> 3 neurons)\n",
        "layer1 = DenseLayer(n_inputs=4, n_neurons=3)\n",
        "layer1.forward(X)  # Compute the output of layer 1\n",
        "\n",
        "# Create the second dense layer (3 inputs -> 2 neurons)\n",
        "layer2 = DenseLayer(n_inputs=3, n_neurons=2)\n",
        "layer2.forward(layer1.output)  # Compute the output of layer 2 using layer 1's output\n",
        "\n",
        "# Print outputs\n",
        "print(\"Output of Layer 1:\")\n",
        "print(layer1.output)\n",
        "print(\"\\nOutput of Layer 2:\")\n",
        "print(layer2.output)"
      ],
      "metadata": {
        "id": "JulVOqLCGjan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation of the Code\n",
        "\n",
        "### 1. Defining Multiple Layers\n",
        "- We extended the `DenseLayer` class to allow the creation of multiple layers**.\n",
        "- Each layer:\n",
        "  - Takes an input.\n",
        "  - Computes the dot product between the input and its weights.\n",
        "  - Adds a bias.\n",
        "  - Passes the result as output to the next layer.\n",
        "\n",
        "\n",
        "\n",
        "### 2. Creating and Connecting Layers\n",
        "#### (a) First Layer (`layer1`)\n",
        "- This layer takes 4 input features and has 3 neurons.\n",
        "- Each neuron in this layer computes:\n",
        "\n",
        "  $$\\text{Layer 1 Output} = X \\cdot W_1 + b_1$$\n",
        "\n",
        "- The output shape of this layer is (samples, 3) because we have 3 neurons.\n",
        "\n",
        "#### (b) Second Layer (`layer2`)\n",
        "- This layer takes 3 inputs (which are the 3 outputs from `layer1`).\n",
        "- It has 2 neurons, so the final output shape is (samples, 2).\n",
        "- The transformation performed by this layer is:\n",
        "\n",
        "  $$\\text{Layer 2 Output} = \\text{Layer 1 Output} \\cdot W_2 + b_2$$\n",
        "\n",
        "- Now, we have a simple multi-layer neural network\n",
        "\n",
        "\n",
        "\n",
        "### 3. Performing a Forward Pass\n",
        "- First, we pass the input data `X` through `layer1` using `layer1.forward(X)`.\n",
        "- Then, we pass `layer1.output` into `layer2` using `layer2.forward(layer1.output)`.\n",
        "- This mimics how deep neural networks pass data from one layer to the next.\n",
        "\n",
        "\n",
        "\n",
        "### 4. Printing the Outputs\n",
        "- The output of `layer1` is displayed first, showing the 3 neuron activations for each sample.\n",
        "- The output of `layer2` is then printed, showing the 2 neuron activations for each sample."
      ],
      "metadata": {
        "id": "oI9WD1-wLics"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1FLNP3kqKfQR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}