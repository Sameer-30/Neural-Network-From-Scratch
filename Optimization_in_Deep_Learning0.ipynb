{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOQL/O2ECkBxP0lPvF9FgmJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sameer-30/Neural-Network-From-Scratch/blob/main/Optimization_in_Deep_Learning0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization in Deep Learning\n",
        "\n",
        "In deep learning, after calculating the loss (or error) of our network, the next critical step is optimization. Optimization refers to the process of updating the model's parameters (weights and biases) in order to minimize the loss function. This notebook will cover:\n",
        "\n",
        "1. Introduction to Optimization\n",
        "2. Gradient Descent and Its Variants\n",
        "3. Advanced Optimization Algorithms (Momentum, AdaGrad, RMSprop, Adam)\n",
        "4. Learning Rate and Scheduling\n",
        "5. Challenges in Optimization\n",
        "6. Code Examples for Understanding the Concepts\n"
      ],
      "metadata": {
        "id": "NdB-8G_c807-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction to Optimization\n",
        "\n",
        "Optimization is the backbone of training neural networks. The goal is to find the set of parameters that minimize the loss function, i.e., the discrepancy between the network's predictions and the ground-truth values. In mathematical terms, we want to solve:\n",
        "\n",
        "$$\n",
        "\\min_{\\theta} \\; L(\\theta)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\theta$ represents all the parameters of the model,\n",
        "- $L(\\theta)$ is the loss function.\n",
        "\n",
        "The most common approach to do this is to use gradient-based methods.\n"
      ],
      "metadata": {
        "id": "5j38t-BO8-7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Gradient Descent and Its Variants\n",
        "\n",
        "### 2.1. Gradient Descent\n",
        "\n",
        "Gradient Descent is the simplest optimization algorithm. It works by computing the gradient (i.e., the derivative) of the loss function with respect to the model parameters and then updating the parameters in the opposite direction of the gradient.\n",
        "\n",
        "The weight update rule is given by:\n",
        "\n",
        "$$\n",
        "\\theta = \\theta - \\eta \\cdot \\nabla_\\theta L(\\theta)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\theta$ represents the parameters,\n",
        "- $\\eta$ is the learning rate, a hyperparameter controlling the step size,\n",
        "- $\\nabla_\\theta L(\\theta)$ is the gradient of the loss function with respect to $\\theta$.\n",
        "\n",
        "### 2.2. Variants of Gradient Descent\n",
        "\n",
        "1. **Batch Gradient Descent:**  \n",
        "   Uses the entire training dataset to compute the gradient in each update. It gives a stable gradient but can be very slow for large datasets.\n",
        "\n",
        "2. **Stochastic Gradient Descent (SGD):**  \n",
        "   Updates the parameters using a single sample (or a very small subset) at each iteration. It introduces noise in the gradient, which can help escape local minima but may also lead to a more erratic path.\n",
        "\n",
        "3. **Mini-Batch Gradient Descent:**  \n",
        "   A compromise between batch and stochastic methods. It updates the parameters using a small batch of samples, offering a balance between the efficiency of SGD and the stability of Batch GD.\n"
      ],
      "metadata": {
        "id": "CLpxQUjI9Ov4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple demonstration of a gradient descent update for a single parameter.\n",
        "# Assume we want to minimize L(w) = (w - 3)^2, whose minimum is at w = 3.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Initialize parameter\n",
        "w = 0.0\n",
        "\n",
        "# Learning rate\n",
        "eta = 0.1\n",
        "\n",
        "# Define the loss function L(w) = (w - 3)^2\n",
        "def loss(w):\n",
        "    return (w - 3)**2\n",
        "\n",
        "# Compute the derivative dL/dw = 2*(w - 3)\n",
        "def grad_loss(w):\n",
        "    return 2 * (w - 3)\n",
        "\n",
        "# Run gradient descent for 10 iterations\n",
        "for i in range(10):\n",
        "    grad = grad_loss(w)\n",
        "    w = w - eta * grad\n",
        "    print(f\"Iteration {i+1}: w = {w:.4f}, loss = {loss(w):.4f}\")\n"
      ],
      "metadata": {
        "id": "-VYrCy7E89GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Advanced Optimization Algorithms\n",
        "\n",
        "While plain gradient descent is conceptually simple, many variants have been developed to address its shortcomings. Here are a few popular ones:\n",
        "\n",
        "### 3.1 Momentum\n",
        "\n",
        "Momentum accelerates SGD by taking past gradients into account, leading to faster convergence and reduced oscillations.\n",
        "\n",
        "Update rule with momentum:\n",
        "\n",
        "$$\n",
        "v := \\gamma v + \\eta \\cdot \\nabla_\\theta L(\\theta)\n",
        "$$\n",
        "$$\n",
        "\\theta := \\theta - v\n",
        "$$\n",
        "\n",
        "where $v$ is the velocity (accumulated gradient) and $\\gamma$ is the momentum coefficient (typically around 0.9).\n",
        "\n",
        "### 3.2 AdaGrad\n",
        "\n",
        "AdaGrad adapts the learning rate for each parameter individually by scaling it inversely proportional to the square root of the sum of all historical squared gradients.\n",
        "\n",
        "Update rule:\n",
        "\n",
        "$$\n",
        "\\theta := \\theta - \\frac{\\eta}{\\sqrt{G + \\epsilon}} \\cdot \\nabla_\\theta L(\\theta)\n",
        "$$\n",
        "\n",
        "where $G$ is the sum of squared gradients and $\\epsilon$ is a small constant to avoid division by zero.\n",
        "\n",
        "### 3.3 RMSprop\n",
        "\n",
        "RMSprop modifies AdaGrad by using a moving average of squared gradients rather than the sum, which helps in non-stationary settings.\n",
        "\n",
        "Update rule:\n",
        "\n",
        "$$\n",
        "E[g^2]_t = \\gamma E[g^2]_{t-1} + (1-\\gamma)(\\nabla_\\theta L(\\theta))^2\n",
        "$$\n",
        "$$\n",
        "\\theta := \\theta - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\cdot \\nabla_\\theta L(\\theta)\n",
        "$$\n",
        "\n",
        "### 3.4 Adam (Adaptive Moment Estimation)\n",
        "\n",
        "Adam combines the benefits of both Momentum and RMSprop by maintaining a moving average of both the gradients and the squared gradients.\n",
        "\n",
        "Update rules:\n",
        "\n",
        "$$\n",
        "m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla_\\theta L(\\theta)\n",
        "$$\n",
        "$$\n",
        "v_t = \\beta_2 v_{t-1} + (1-\\beta_2)(\\nabla_\\theta L(\\theta))^2\n",
        "$$\n",
        "$$\n",
        "\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}\n",
        "$$\n",
        "$$\n",
        "\\theta := \\theta - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
        "$$\n",
        "\n",
        "Adam is widely used due to its robustness and efficiency across a variety of deep learning applications.\n"
      ],
      "metadata": {
        "id": "vJYwPeVH-Co2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Learning Rate and Scheduling\n",
        "\n",
        "The learning rate  $\\eta$ is one of the most crucial hyperparameters. If it is too large, the algorithm may overshoot the minimum; if too small, the training can become painfully slow or get stuck in local minima.\n",
        "\n",
        "### Learning Rate Scheduling\n",
        "\n",
        "It is often beneficial to adjust the learning rate during training:\n",
        "- **Step Decay:** Reduce the learning rate by a factor every few epochs.\n",
        "- **Exponential Decay:** Multiply the learning rate by a factor \\(\\alpha < 1\\) at each iteration.\n",
        "- **Adaptive Methods:** Methods like Adam adapt the learning rate for each parameter automatically.\n",
        "\n",
        "For example, an exponential decay can be written as:\n",
        "\n",
        "$$\n",
        "\\eta_t = \\eta_0 \\cdot \\alpha^t\n",
        "$$\n",
        "\n",
        "where $\\eta_0$ is the initial learning rate and $\\alpha$ is a decay factor.\n"
      ],
      "metadata": {
        "id": "z8Gev5iF-0ML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Exponential learning rate decay in a training loop\n",
        "initial_lr = 0.1\n",
        "decay_rate = 0.95  # Decay the learning rate by 5% each epoch\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    current_lr = initial_lr * (decay_rate ** epoch)\n",
        "    print(f\"Epoch {epoch+1}: Learning Rate = {current_lr:.5f}\")\n"
      ],
      "metadata": {
        "id": "pKw40xbd99EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Challenges in Optimization\n",
        "\n",
        "During training, several challenges may arise:\n",
        "- **Local Minima and Saddle Points:** The loss surface may contain many local minima or saddle points where gradients are very small.\n",
        "- **Vanishing/Exploding Gradients:** In deep networks, gradients can become too small (vanishing) or too large (exploding), making training unstable.\n",
        "- **Overfitting:** The model may fit the training data too closely, failing to generalize. Techniques like regularization and dropout are used to combat this.\n",
        "\n",
        "Understanding these challenges is essential for choosing the right optimization strategy and tuning hyperparameters effectively.\n"
      ],
      "metadata": {
        "id": "7oqZSQQY_KbN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BPbFNwZB_Iha"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}