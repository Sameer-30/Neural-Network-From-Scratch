{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMEvEGXU4qou5fFgL1MuKe+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sameer-30/Neural-Network-From-Scratch/blob/main/Activation_Functions_in_Neural_Networks0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Functions in Neural Network\n",
        "\n",
        "In this notebook, we explore the importance of activation functions in deep learning, their mathematical foundations, and code implementations. We'll cover:\n",
        "\n",
        "1. **Introduction to Activation Functions**\n",
        "2. **Types of Activation Functions**\n",
        "    - Step Function\n",
        "    - Linear Function\n",
        "    - Sigmoid Function\n",
        "    - Rectified Linear Unit (ReLU)\n",
        "    - Softmax Function\n",
        "3. **Implementing Activation Functions in Python**\n",
        "4. **Building a Simple Neural Network Example**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wekVEaIoucPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction\n",
        "\n",
        "Activation functions are essential components in neural networks. They introduce non-linearity, allowing the network to learn complex patterns. Without them, even a deep network would behave like a single linear model.\n",
        "\n",
        "In our deep learning models:\n",
        "- **Hidden Layers** typically use non-linear activations (like ReLU or Sigmoid) to capture intricate patterns.\n",
        "- **Output Layers** use activations tailored to the task (Softmax for classification, Linear for regression).\n"
      ],
      "metadata": {
        "id": "BhNpM6ARwF9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Types of Activation Functions\n",
        "\n",
        "### 2.1 Step Activation Function\n",
        "\n",
        "The step function mimics a neuron \"firing\" or \"not firing\":\n",
        "$$f(x) = \\begin{cases}\n",
        "1 & \\text{if } x > 0 \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        " In a single neuron, if\n",
        "the weights · inputs + bias​ results in a value greater than 0, the neuron will fire and output a 1;\n",
        "otherwise, it will output a 0.\n",
        "\n",
        "\n",
        "*Note:* It provides no gradient information, which is why it is rarely used in modern deep learning.\n"
      ],
      "metadata": {
        "id": "-hVDTOsuwUGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Linear Activation Function\n",
        "\n",
        "The linear function is defined as:\n",
        "$$\n",
        "f(x) = x\n",
        "$$\n",
        "It is often used in regression problems. However, stacking multiple linear layers still yields a linear function.\n"
      ],
      "metadata": {
        "id": "j5ngF_ZywZXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Sigmoid Activation Function\n",
        "\n",
        "The Sigmoid function is given by:\n",
        "$$\n",
        "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
        "$$\n",
        "It outputs values between 0 and 1, making it suitable for binary classification. However, it can suffer from vanishing gradients.\n"
      ],
      "metadata": {
        "id": "0n_20iipxskR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Rectified Linear Unit (ReLU)\n",
        "\n",
        "ReLU is defined as:\n",
        "$$\n",
        "f(x) = \\max(0, x)\n",
        "$$\n",
        "ReLU is widely used because it is computationally efficient and helps mitigate the vanishing gradient problem.\n",
        "\n",
        "#### Code Examples for ReLU:\n"
      ],
      "metadata": {
        "id": "1-g7HUsJx8mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU using a loop\n",
        "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
        "output = []\n",
        "for i in inputs:\n",
        "    if i > 0:\n",
        "        output.append(i)\n",
        "    else:\n",
        "        output.append(0)\n",
        "print(\"ReLU output (loop):\", output)\n"
      ],
      "metadata": {
        "id": "0_aqp_MkyMmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU using list comprehension\n",
        "output = [max(0, i) for i in inputs]\n",
        "print(\"ReLU output (comprehension):\", output)\n"
      ],
      "metadata": {
        "id": "74hNgQTQyNwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU using NumPy\n",
        "import numpy as np\n",
        "output = np.maximum(0, inputs)\n",
        "print(\"ReLU output (NumPy):\", output)\n"
      ],
      "metadata": {
        "id": "xQPubSMAyWWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5. Softmax Activation Function\n",
        "\n",
        "Softmax converts logits into probabilities:\n",
        "$$\n",
        "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
        "$$\n",
        "\n",
        "For numerical stability, we subtract the maximum value:\n",
        "$$\n",
        "\\text{softmax}(z_i) = \\frac{e^{z_i - \\max(z)}}{\\sum_j e^{z_j - \\max(z)}}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "bpMQvO8ryjdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax example for a single sample\n",
        "layer_outputs = [4.8, 1.21, 2.385]\n",
        "exp_values = np.exp(layer_outputs - np.max(layer_outputs))\n",
        "norm_values = exp_values / np.sum(exp_values)\n",
        "\n",
        "print(\"Exponentiated values:\", exp_values)\n",
        "print(\"Softmax output:\", norm_values)\n"
      ],
      "metadata": {
        "id": "e4Jc2x-Kyfbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax example for a batch of data\n",
        "inputs = np.array([[4.8, 1.21, 2.385],\n",
        "                   [8.9, -1.81, 0.2],\n",
        "                   [1.41, 1.051, 0.026]])\n",
        "\n",
        "exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "print(\"Batch Softmax output:\\n\", probabilities)\n"
      ],
      "metadata": {
        "id": "NDICA83Lyvyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Implementing Activation Functions in a Neural Network\n",
        "\n",
        "Let's build a simple neural network with:\n",
        "- A dense (fully-connected) layer\n",
        "- ReLU activation in the hidden layer\n",
        "- Softmax activation in the output layer\n",
        "\n",
        "### 3.1 Dense Layer\n",
        "The dense layer computes:\n",
        "$$\n",
        "\\text{Output} = XW + b\n",
        "$$\n"
      ],
      "metadata": {
        "id": "TfbHRua8y4RP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense Layer class\n",
        "class Layer_Dense:\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        # Initialize weights with small random values and biases with zeros\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Compute the linear combination of inputs, weights, and biases\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n"
      ],
      "metadata": {
        "id": "aAxUfntSyzv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 ReLU Activation Layer\n"
      ],
      "metadata": {
        "id": "ZB8l75yWzOO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU Activation class\n",
        "class Activation_ReLU:\n",
        "    def forward(self, inputs):\n",
        "        # Apply the ReLU function element-wise\n",
        "        self.output = np.maximum(0, inputs)\n"
      ],
      "metadata": {
        "id": "UGlC5u6JzEU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Softmax Activation Layer\n"
      ],
      "metadata": {
        "id": "P0yFkj-kzXZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Activation class\n",
        "class Activation_Softmax:\n",
        "    def forward(self, inputs):\n",
        "        # Subtract the maximum value for numerical stability\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "        # Normalize to get probabilities\n",
        "        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n"
      ],
      "metadata": {
        "id": "NflZF6tvzU93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Building and Running a Simple Neural Network\n",
        "\n",
        "Below is the complete code that creates a simple neural network, passes data through its layers, and prints the output probabilities. This demonstrates how the network uses activation functions to produce predictions.\n"
      ],
      "metadata": {
        "id": "4m4JTIgmzf5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nnfs\n",
        "from nnfs.datasets import spiral_data # Import the spiral_data function\n",
        "# Generate the spiral dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create first dense layer (input layer)\n",
        "dense1 = Layer_Dense(2, 3)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second dense layer (output layer)\n",
        "dense2 = Layer_Dense(3, 3)\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "# Forward pass through the network\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "# Display the first 5 output probabilities\n",
        "print(\"Network output (first 5 samples):\\n\", activation2.output[:5])\n"
      ],
      "metadata": {
        "id": "CrfKc9Nazabi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SSvt2LDjzmRr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}