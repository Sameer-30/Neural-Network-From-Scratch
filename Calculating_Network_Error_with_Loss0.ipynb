{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNVf23pFWJLRwjsMQDGap2Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sameer-30/Neural-Network-From-Scratch/blob/main/Calculating_Network_Error_with_Loss0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculating Categorical Cross-Entropy Loss & Accuracy\n",
        "\n",
        "In this notebook, we demonstrate how to compute the categorical cross-entropy loss and accuracy for a neural network. Unlike previous examples, we use synthetic data to illustrate the steps in detail.\n",
        "\n",
        "We'll cover:\n",
        "- Generating synthetic softmax predictions for a batch of samples.\n",
        "- Calculating the loss using sparse (index-based) targets.\n",
        "- Converting targets to one-hot encoded format and calculating the loss.\n",
        "- Computing the accuracy metric.\n",
        "\n"
      ],
      "metadata": {
        "id": "Hh2-USnO3jG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate synthetic softmax predictions for a batch of 5 samples with 4 classes.\n",
        "# These predictions might come from a softmax layer in a neural network.\n",
        "predictions = np.array([\n",
        "    [0.85, 0.05, 0.05, 0.05],\n",
        "    [0.10, 0.70, 0.10, 0.10],\n",
        "    [0.20, 0.20, 0.50, 0.10],\n",
        "    [0.30, 0.30, 0.20, 0.20],\n",
        "    [0.05, 0.15, 0.70, 0.10]\n",
        "])\n",
        "\n",
        "# Assume the true class for each sample is given as an index (sparse targets).\n",
        "# For example, sample 0's true label is class 0, sample 1's is class 1, etc.\n",
        "target_labels = np.array([0, 1, 2, 0, 2])\n",
        "\n",
        "print(\"Synthetic Predictions:\\n\", predictions)\n",
        "print(\"Target Labels (sparse):\", target_labels)\n"
      ],
      "metadata": {
        "id": "NX3N1TCB4gAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Calculating Categorical Cross-Entropy Loss (Sparse Targets)\n",
        "In many classification tasks, the neural network outputs a probability distribution over classes using a softmax activation. For each sample, the categorical cross-entropy loss is defined as:\n",
        "$$\n",
        "L = -\\log(\\hat{y}_k)\n",
        "$$\n",
        "where $\\hat{y}_k$ is the predicted probability for the true class $(k)$.\n",
        "\n",
        "\n",
        "When we have a batch of samples, our predicted probabilities are stored in a two-dimensional array of shape\n",
        "(\n",
        "batch_size\n",
        ",\n",
        "num_classes\n",
        ")\n",
        "(batch_size,num_classes). The true labels are given as a one-dimensional (sparse) array where each element is the index of the correct class.\n",
        "\n",
        "Consider the following function that implements this loss calculation:"
      ],
      "metadata": {
        "id": "rMUo0k_Q3tBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categorical_crossentropy_loss(y_pred, y_true):\n",
        "\n",
        "    # Clip predictions to avoid log(0) issues\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "    # Extract the predicted probabilities for the correct classes\n",
        "    # np.arange(len(y_pred)) creates an array of sample indices\n",
        "    correct_confidences = y_pred_clipped[np.arange(len(y_pred)), y_true]\n",
        "\n",
        "    # Compute the negative log likelihood for each sample\n",
        "    negative_log_likelihoods = -np.log(correct_confidences)\n",
        "\n",
        "    # Return the mean loss over the batch\n",
        "    return np.mean(negative_log_likelihoods)\n",
        "\n",
        "loss_sparse = categorical_crossentropy_loss(predictions, target_labels)\n",
        "print(\"Categorical Cross-Entropy Loss (sparse targets):\", loss_sparse)\n"
      ],
      "metadata": {
        "id": "51C55UYZ30Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Handling One-Hot Encoded Targets\n",
        "\n",
        "Sometimes targets are provided as one-hot encoded vectors. For example, if the target label for a sample is class 2 among 4 classes, its one-hot representation is $[0, 0, 1, 0]$.\n",
        "\n",
        "To calculate the loss, we multiply the predicted probabilities by the one-hot encoded target and sum along the class dimension.\n"
      ],
      "metadata": {
        "id": "mOWQ602D5bEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_one_hot(labels, num_classes):\n",
        "\n",
        "    one_hot = np.zeros((labels.shape[0], num_classes))\n",
        "    one_hot[np.arange(labels.shape[0]), labels] = 1\n",
        "    return one_hot\n",
        "\n",
        "# Convert our sparse target_labels to one-hot encoded format.\n",
        "target_one_hot = to_one_hot(target_labels, 4)\n",
        "print(\"One-Hot Encoded Targets:\\n\", target_one_hot)\n"
      ],
      "metadata": {
        "id": "DcFlfGy_5O-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categorical_crossentropy_loss_onehot(y_pred, y_true):\n",
        "       # Clip predictions for numerical stability\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "    # For one-hot targets, multiply element-wise and sum along classes to get correct confidences\n",
        "    correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
        "\n",
        "    # Compute the negative log likelihood\n",
        "    negative_log_likelihoods = -np.log(correct_confidences)\n",
        "\n",
        "    return np.mean(negative_log_likelihoods)\n",
        "\n",
        "loss_onehot = categorical_crossentropy_loss_onehot(predictions, target_one_hot)\n",
        "print(\"Categorical Cross-Entropy Loss (one-hot targets):\", loss_onehot)\n"
      ],
      "metadata": {
        "id": "awi82GlZ5oaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Calculating Accuracy\n",
        "\n",
        "Accuracy is determined by comparing the predicted class (the index with the highest probability) with the true class.\n",
        "\n",
        "For a batch of predictions, we compute:\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of samples}}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "uCSxSewa5wyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(y_pred, y_true):\n",
        "        # Determine the predicted class for each sample\n",
        "    pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    # If y_true is one-hot encoded, convert to sparse format\n",
        "    if len(y_true.shape) == 2:\n",
        "        y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "    accuracy = np.mean(pred_labels == y_true)\n",
        "    return accuracy\n",
        "\n",
        "accuracy_val = calculate_accuracy(predictions, target_labels)\n",
        "print(\"Accuracy (using sparse targets):\", accuracy_val)\n"
      ],
      "metadata": {
        "id": "jwX_GA585sTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aG5SkOgN57xB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}